# src/modeling/train.py (æ›´æ–°ç‰ˆ)

import os
import sys
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import h5py
from tqdm import tqdm

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src import config
from src.modeling.model import build_adaptive_cnn_model # å¯¼å…¥æ–°æ¨¡å‹
from src.modeling.dataset import create_dataset
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from src.utils.callbacks import OneCycleLR # --- æ–°å¢ ---

def stratified_sample(csi_labels, num_samples):
    """æ‰§è¡Œåˆ†å±‚é‡‡æ ·"""
    df = pd.DataFrame({'csi': csi_labels})
    bins = pd.cut(df['csi'], bins=[0, 0.2, 0.4, 0.7, 1.0], right=False, labels=False)
    
    # æŒ‰æ¯”ä¾‹åˆ†é…æ ·æœ¬
    df_train, _ = train_test_split(
        df, 
        train_size=num_samples, 
        stratify=bins, 
        random_state=42
    )
    return sorted(df_train.index.tolist())

def get_normalization_stats(h5_path, train_indices):
    """è®¡ç®—è®­ç»ƒé›†çš„log1pæ ‡å‡†åŒ–ç»Ÿè®¡é‡"""
    print("  Calculating normalization stats from training data...")
    with h5py.File(h5_path, 'r') as hf:
        scalograms = hf['scalograms']
        
        # ä¸ºäº†èŠ‚çº¦å†…å­˜ï¼Œåˆ†æ‰¹è®¡ç®—
        sums = 0.0
        sum_sqs = 0.0
        total_pixels = 0
        
        for i in tqdm(train_indices, desc="  Stats Calculation"):
            data = np.log1p(scalograms[i,:,:])
            sums += data.sum()
            sum_sqs += np.sum(np.square(data))
            total_pixels += data.size
            
    mean = sums / total_pixels
    std = np.sqrt(sum_sqs / total_pixels - np.square(mean))
    
    print(f"  - Calculated Mean: {mean:.4f}, Std: {std:.4f}")
    return mean, std

def run_training():
    """æ‰§è¡Œå®Œæ•´çš„ã€ç»è¿‡ä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒæµç¨‹"""
    if not os.path.exists(config.SCALOGRAM_DATA_PATH):
        print(f"Error: Scalogram data file not found at {config.SCALOGRAM_DATA_PATH}")
        return
        
    print("\n--- Starting Optimized Model Training ---")

    # 1. åŠ è½½å®Œæ•´æ•°æ®é›†ä¿¡æ¯å¹¶åº”ç”¨é‡‡æ ·ç­–ç•¥
    with h5py.File(config.SCALOGRAM_DATA_PATH, 'r') as hf:
        num_total_samples = len(hf['csi_labels'])
        all_csi_labels = hf['csi_labels'][:]
        input_shape = hf['scalograms'].shape[1:]

    if config.QUICK_TEST_MODE:
        print(f"ğŸš€ Running in QUICK TEST mode with {config.QUICK_TEST_SAMPLES} samples.")
        from sklearn.model_selection import train_test_split
        # ä½¿ç”¨åˆ†å±‚é‡‡æ ·è·å–æµ‹è¯•å­é›†
        sample_indices, _ = train_test_split(np.arange(num_total_samples), 
                                             train_size=config.QUICK_TEST_SAMPLES,
                                             stratify=pd.cut(all_csi_labels, bins=4),
                                             random_state=42)
    else:
        print("ğŸ’ª Running in FULL DATASET mode.")
        sample_indices = np.arange(num_total_samples)
    
    # 2. åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    from sklearn.model_selection import train_test_split
    train_indices, val_indices = train_test_split(sample_indices, 
                                                  test_size=0.2, 
                                                  random_state=42,
                                                  stratify=pd.cut(all_csi_labels[sample_indices], bins=4))

    n_train = len(train_indices)
    n_val = len(val_indices)
    print(f"Total samples for this run: {n_train + n_val}")
    print(f"Training samples: {n_train}, Validation samples: {n_val}")

    # 3. è®¡ç®—æ ‡å‡†åŒ–æ‰€éœ€çš„å‡å€¼å’Œæ ‡å‡†å·® (ä»…ä½¿ç”¨è®­ç»ƒé›†)
    mean, std = get_normalization_stats(config.SCALOGRAM_DATA_PATH, train_indices)
    
    # 4. æ ¹æ®æ ·æœ¬é‡åŠ¨æ€è®¾ç½®è¶…å‚æ•°
    if n_train <= 5000:
        epochs = 30 # å¯ä»¥é€‚å½“å¢åŠ å‘¨æœŸï¼Œå› ä¸ºOneCycleLRèƒ½æœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆ
        batch_size = 32
        patience = 8 # æ—©åœè€å¿ƒå€¼
    else: # å¤§æ•°æ®é›†
        epochs = 50
        batch_size = 64
        patience = 10
        
    # --- æ–°å¢ï¼šåœ¨è¿™é‡Œè®¾ç½®ä»LR Finderæ‰¾åˆ°çš„æœ€ä½³å­¦ä¹ ç‡ ---
    # æ ¹æ®ç»éªŒï¼Œ1e-3é€šå¸¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹
    MAX_LR = 1e-3 
    print(f"  Using OneCycleLR with max_lr: {MAX_LR}")
    print(f"  Dynamically set hyperparameters: epochs={epochs}, batch_size={batch_size}")

    # 5. åˆ›å»ºæ•°æ®é›†
    train_dataset = create_dataset(config.SCALOGRAM_DATA_PATH, train_indices, batch_size, mean, std, is_training=True)
    val_dataset = create_dataset(config.SCALOGRAM_DATA_PATH, val_indices, batch_size, mean, std, is_training=False)

    # 6. æ„å»ºå¹¶ç¼–è¯‘æ¨¡å‹
    model = build_adaptive_cnn_model(input_shape, n_train)
    # æ³¨æ„ï¼šOneCycleLRä¼šç®¡ç†å­¦ä¹ ç‡ï¼Œæ‰€ä»¥è¿™é‡ŒAdamçš„lrå‚æ•°ä¸é‡è¦
    model.compile(
        optimizer=tf.keras.optimizers.Adam(), 
        loss='mean_squared_error',
        metrics=['mean_absolute_error']
    )
    model.summary()

    # 7. è®¾ç½®å›è°ƒå‡½æ•° (ä½¿ç”¨OneCycleLR)
    steps_per_epoch = n_train // batch_size
    
    callbacks = [
        ModelCheckpoint(
            filepath=config.BEST_MODEL_PATH,
            save_best_only=True,
            monitor='val_loss',
            mode='min',
            verbose=1
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=patience,
            mode='min',
            restore_best_weights=True,
            verbose=1
        ),
        # --- ä½¿ç”¨æ–°å›è°ƒï¼Œæ›¿æ¢æ‰ReduceLROnPlateau ---
        OneCycleLR(
            max_lr=MAX_LR,
            steps_per_epoch=steps_per_epoch,
            epochs=epochs
        )
    ]

    # 8. å¼€å§‹è®­ç»ƒ
    print("\nStarting training...")
    model.fit(
        train_dataset,
        epochs=epochs,
        validation_data=val_dataset,
        callbacks=callbacks
    )

    print("\n--- Optimized Training Finished ---")
    print(f"Best model saved to: {config.BEST_MODEL_PATH}")

if __name__ == '__main__':
    run_training()